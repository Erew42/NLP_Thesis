{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEC pipeline runner\n",
        "\n",
        "Run the core steps in `thesis_pkg.pipelines.sec_pipeline` on a dataset of yearly SEC filing ZIPs.\n",
        "\n",
        "Update the paths in the setup cell before running.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Make local `src` importable when running from repo checkout\n",
        "ROOT = Path.cwd().resolve()\n",
        "SRC = ROOT / \"src\"\n",
        "if SRC.exists():\n",
        "    sys.path.insert(0, str(SRC))\n",
        "\n",
        "from thesis_pkg.filing_text import (\n",
        "    build_light_metadata_dataset,\n",
        "    merge_yearly_batches,\n",
        "    process_zip_year,\n",
        "    process_zip_year_raw_text,\n",
        "    process_year_dir_extract_items,\n",
        "    summarize_year_parquets,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ---- Paths (edit these) ----\n",
        "zip_dir = Path(r\"C:\\\\path\\\\to\\\\sec_zip_dir\")  # directory of yearly .zip archives\n",
        "batch_dir = zip_dir / \"parquet_batches\"\n",
        "merge_dir = zip_dir / \"year_merged\"\n",
        "tmp_dir = batch_dir / \"_tmp\"\n",
        "light_path = zip_dir / \"filings_metadata_LIGHT.parquet\"\n",
        "checkpoint = merge_dir / \"done_years.json\"\n",
        "\n",
        "# ---- Tuning ----\n",
        "batch_max_rows = 1000\n",
        "batch_max_text_bytes = 250 * 1024 * 1024\n",
        "compression = \"zstd\"\n",
        "compression_level = 1\n",
        "sleep_between_years = 0.0\n",
        "overwrite_batches = False\n",
        "use_parsed_headers = False  # switch to True to run process_zip_year\n",
        "\n",
        "batch_dir.mkdir(parents=True, exist_ok=True)\n",
        "merge_dir.mkdir(parents=True, exist_ok=True)\n",
        "tmp_dir.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Step 1: process zip archives into parquet batches\n",
        "for zip_path in sorted(zip_dir.glob(\"*.zip\")):\n",
        "    existing = list(batch_dir.glob(f\"{zip_path.stem}_batch_*.parquet\"))\n",
        "    if existing and not overwrite_batches:\n",
        "        print(f\"[skip] batches exist for {zip_path.name}\")\n",
        "        continue\n",
        "    if existing:\n",
        "        for p in existing:\n",
        "            p.unlink()\n",
        "\n",
        "    print(f\"[batch] {zip_path.name}\")\n",
        "    if use_parsed_headers:\n",
        "        process_zip_year(\n",
        "            zip_path=zip_path,\n",
        "            out_dir=batch_dir,\n",
        "            batch_max_rows=batch_max_rows,\n",
        "            batch_max_text_bytes=batch_max_text_bytes,\n",
        "            tmp_dir=tmp_dir,\n",
        "            compression=compression,\n",
        "        )\n",
        "    else:\n",
        "        process_zip_year_raw_text(\n",
        "            zip_path=zip_path,\n",
        "            out_dir=batch_dir,\n",
        "            batch_max_rows=batch_max_rows,\n",
        "            batch_max_text_bytes=batch_max_text_bytes,\n",
        "            tmp_dir=tmp_dir,\n",
        "            compression=compression,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Step 2: merge yearly batches into per-year parquet files\n",
        "merged_paths = merge_yearly_batches(\n",
        "    batch_dir=batch_dir,\n",
        "    out_dir=merge_dir,\n",
        "    checkpoint_path=checkpoint,\n",
        "    batch_size=32_000,\n",
        "    compression=compression,\n",
        "    compression_level=compression_level,\n",
        "    sleep_between_years=sleep_between_years,\n",
        ")\n",
        "print(f\"[merge] merged {len(merged_paths)} yearly files into {merge_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Step 3: summarize merged files\n",
        "summary = summarize_year_parquets(merge_dir)\n",
        "print(\"[summary] year rows status\")\n",
        "for item in summary:\n",
        "    print(f\"  {item['year']}: {item['rows']} rows ({item['status']})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Step 4: build metadata-only parquet (drops full_text)\n",
        "build_light_metadata_dataset(\n",
        "    parquet_dir=merge_dir,\n",
        "    out_path=light_path,\n",
        "    drop_columns=(\"full_text\",),\n",
        "    sort_columns=(\"file_date_filename\", \"cik\"),\n",
        "    compression=compression,\n",
        ")\n",
        "print(f\"[light] wrote {light_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: extract items from merged yearly parquets\n",
        "\n",
        "Uncomment and run this if you want item-level extraction after merging.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# items_dir = merge_dir / \"items\"\n",
        "# items_dir.mkdir(parents=True, exist_ok=True)\n",
        "#\n",
        "# process_year_dir_extract_items(\n",
        "#     year_dir=merge_dir,\n",
        "#     out_dir=items_dir,\n",
        "#     years=None,\n",
        "#     parquet_batch_rows=16,\n",
        "#     out_batch_max_rows=50_000,\n",
        "#     out_batch_max_text_bytes=250 * 1024 * 1024,\n",
        "#     tmp_dir=tmp_dir,\n",
        "#     compression=compression,\n",
        "#     local_work_dir=tmp_dir / \"_item_work\",\n",
        "#     non_item_diagnostic=False,\n",
        "#     include_full_text=False,\n",
        "#     regime=True,\n",
        "# )\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}