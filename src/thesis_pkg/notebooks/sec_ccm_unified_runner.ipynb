{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SEC-CCM Unified Runner (Colab-first)\n",
        "\n",
        "This notebook unifies the old CCM merge and SEC filing workflows.\n",
        "\n",
        "Execution order (locked):\n",
        "1. Build/reuse CCM daily panel\n",
        "2. Parse/merge SEC filings\n",
        "3. Run SEC-CCM pre-merge (doc grain)\n",
        "4. Run gated item extraction using matched `doc_id` allowlist\n",
        "5. Run diagnostics (run report + unmatched + no-item + boundary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "ROOT = Path.cwd().resolve()\n",
        "SRC = ROOT / \"src\"\n",
        "if SRC.exists() and str(SRC) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC))\n",
        "\n",
        "print({\"IN_COLAB\": IN_COLAB, \"ROOT\": str(ROOT), \"SRC_EXISTS\": SRC.exists()})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "from pathlib import Path\n",
        "\n",
        "from thesis_pkg.pipeline import (\n",
        "    SecCcmJoinSpecV1,\n",
        "    build_or_reuse_ccm_daily_stage,\n",
        "    run_sec_ccm_premerge_pipeline,\n",
        ")\n",
        "from thesis_pkg.filing_text import (\n",
        "    process_zip_year_raw_text,\n",
        "    process_zip_year,\n",
        "    merge_yearly_batches,\n",
        "    summarize_year_parquets,\n",
        "    build_light_metadata_dataset,\n",
        "    process_year_dir_extract_items_gated,\n",
        "    compute_no_item_diagnostics,\n",
        ")\n",
        "from thesis_pkg.core.sec.suspicious_boundary_diagnostics import (\n",
        "    DiagnosticsConfig,\n",
        "    parse_focus_items,\n",
        "    run_boundary_diagnostics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Config\n",
        "\n",
        "Edit these toggles and paths first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RUN_CCM_MODE = \"REUSE\"  # REBUILD | REUSE\n",
        "RUN_SEC_PARSE = False\n",
        "RUN_SEC_YEARLY_MERGE = True\n",
        "RUN_SEC_CCM_PREMERGE = True\n",
        "RUN_GATED_ITEM_EXTRACTION = True\n",
        "RUN_UNMATCHED_DIAGNOSTIC_TRACK = False\n",
        "RUN_NO_ITEM_DIAGNOSTICS = True\n",
        "RUN_BOUNDARY_DIAGNOSTICS = True\n",
        "RUN_VALIDATION_CHECKS = True\n",
        "\n",
        "SEC_PARSE_MODE = \"parsed\"  # raw | parsed\n",
        "YEARS = list(range(1993, 2025))\n",
        "ITEM_EXTRACTION_REGIME = \"legacy\"\n",
        "\n",
        "if IN_COLAB:\n",
        "    WORK_ROOT = Path(\"/content/drive/MyDrive/Data_LM\")\n",
        "else:\n",
        "    WORK_ROOT = Path(\"C:/Users/erik9/Documents/SEC_Data\")\n",
        "\n",
        "SEC_ZIP_DIR = WORK_ROOT / \"Data\" / \"Sample_Filings\"\n",
        "SEC_BATCH_ROOT = WORK_ROOT / \"Data\" / \"Sample_Filings\" / \"parquet_batches\"\n",
        "SEC_YEAR_MERGED_DIR = SEC_BATCH_ROOT / \"_year_merged\"\n",
        "SEC_LIGHT_METADATA_PATH = WORK_ROOT / \"Data\" / \"Sample_Filings\" / \"filings_metadata_LIGHT.parquet\"\n",
        "\n",
        "CCM_BASE_DIR = WORK_ROOT / \"Data\" / \"CRSP_Compustat_data\" / \"parquet_data\"\n",
        "CCM_DERIVED_DIR = WORK_ROOT / \"Data\" / \"CRSP_Compustat_data\" / \"derived_data\"\n",
        "CCM_REUSE_DAILY_PATH = WORK_ROOT / \"Data\" / \"Sample_CCM\" / \"final_flagged_data_compdesc_added.sample_3pct_seed42_compdesc.parquet\"\n",
        "\n",
        "RUN_ROOT = ROOT / \"results\" / \"sec_ccm_unified_runner\"\n",
        "SEC_CCM_OUTPUT_DIR = RUN_ROOT / \"sec_ccm_premerge\"\n",
        "SEC_ITEMS_ANALYSIS_DIR = RUN_ROOT / \"items_analysis\"\n",
        "SEC_ITEMS_DIAGNOSTIC_DIR = RUN_ROOT / \"items_diagnostic\"\n",
        "SEC_NO_ITEM_DIR = RUN_ROOT / \"no_item_diagnostics\"\n",
        "BOUNDARY_OUT_DIR = RUN_ROOT / \"boundary_diagnostics\"\n",
        "BOUNDARY_INPUT_DIR = BOUNDARY_OUT_DIR / \"matched_filings_input\"\n",
        "\n",
        "if IN_COLAB:\n",
        "    LOCAL_TMP = Path(\"/content/_tmp_zip\")\n",
        "    LOCAL_WORK = Path(\"/content/_batch_work\")\n",
        "    LOCAL_ITEM_WORK = Path(\"/content/_item_work\")\n",
        "    LOCAL_MERGE_WORK = Path(\"/content/_merge_work\")\n",
        "else:\n",
        "    LOCAL_TMP = ROOT / \".tmp\" / \"zip\"\n",
        "    LOCAL_WORK = ROOT / \".tmp\" / \"batch_work\"\n",
        "    LOCAL_ITEM_WORK = ROOT / \".tmp\" / \"item_work\"\n",
        "    LOCAL_MERGE_WORK = ROOT / \".tmp\" / \"merge_work\"\n",
        "\n",
        "for p in [SEC_BATCH_ROOT, SEC_YEAR_MERGED_DIR, RUN_ROOT, SEC_CCM_OUTPUT_DIR, SEC_ITEMS_ANALYSIS_DIR, SEC_ITEMS_DIAGNOSTIC_DIR, SEC_NO_ITEM_DIR, BOUNDARY_OUT_DIR, BOUNDARY_INPUT_DIR, LOCAL_TMP, LOCAL_WORK, LOCAL_ITEM_WORK, LOCAL_MERGE_WORK]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FORMS_10K_10Q = [\"10-K\", \"10-K/A\", \"10-KA\", \"10-Q\", \"10-Q/A\", \"10-QA\", \"10-KT\", \"10-KT/A\", \"10-QT\", \"10-QT/A\", \"10-K405\"]\n",
        "DAILY_FEATURE_COLUMNS = (\"RET\", \"RETX\", \"PRC\", \"BIDLO\", \"ASKHI\", \"VOL\")\n",
        "REQUIRED_DAILY_NON_NULL_FEATURES = (\"RET\",)\n",
        "\n",
        "print({\"RUN_CCM_MODE\": RUN_CCM_MODE, \"WORK_ROOT\": str(WORK_ROOT), \"RUN_ROOT\": str(RUN_ROOT)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) CCM stage (build or reuse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ccm_stage_paths = build_or_reuse_ccm_daily_stage(\n",
        "    run_mode=RUN_CCM_MODE,\n",
        "    ccm_base_dir=CCM_BASE_DIR,\n",
        "    ccm_derived_dir=CCM_DERIVED_DIR,\n",
        "    ccm_reuse_daily_path=Path(CCM_REUSE_DAILY_PATH),\n",
        "    forms_10k_10q=FORMS_10K_10Q,\n",
        "    start_date=\"1990-01-01\",\n",
        "    canonical_name=\"canonical_link_table.parquet\",\n",
        "    daily_name=\"final_flagged_data_compdesc_added.parquet\",\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "ccm_daily_path = ccm_stage_paths[\"ccm_daily_path\"]\n",
        "canonical_link_path = ccm_stage_paths[\"canonical_link_path\"]\n",
        "\n",
        "ccm_daily_lf = pl.scan_parquet(ccm_daily_path)\n",
        "print({\"ccm_daily_path\": str(ccm_daily_path), \"canonical_link_path\": str(canonical_link_path), \"rows\": ccm_daily_lf.select(pl.len()).collect().item()})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Build link universe + trading calendar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _first_existing(schema: pl.Schema, candidates: tuple[str, ...], label: str) -> str:\n",
        "    for c in candidates:\n",
        "        if c in schema:\n",
        "            return c\n",
        "    raise ValueError(f\"{label} missing candidates: {list(candidates)}\")\n",
        "\n",
        "\n",
        "schema = ccm_daily_lf.collect_schema()\n",
        "resolved_permno_col = _first_existing(schema, (\"KYPERMNO\", \"LPERMNO\", \"PERMNO\"), \"ccm_daily\")\n",
        "resolved_date_col = _first_existing(schema, (\"CALDT\", \"caldt\"), \"ccm_daily\")\n",
        "\n",
        "link_universe_lf = pl.scan_parquet(canonical_link_path)\n",
        "trading_calendar_lf = ccm_daily_lf.select(pl.col(resolved_date_col).cast(pl.Date, strict=False).alias(\"CALDT\")).drop_nulls(subset=[\"CALDT\"]).unique().sort(\"CALDT\")\n",
        "\n",
        "print({\"permno_col\": resolved_permno_col, \"date_col\": resolved_date_col, \"canonical_link_path\": str(canonical_link_path)})\n",
        "print({\"link_rows\": link_universe_lf.select(pl.len()).collect().item(), \"trading_days\": trading_calendar_lf.select(pl.len()).collect().item()})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) SEC parse and yearly merge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RUN_SEC_PARSE:\n",
        "    common = dict(tmp_dir=LOCAL_TMP, local_work_dir=LOCAL_WORK, compression=\"zstd\", copy_retries=5, copy_sleep=2.0, validate_on_copy=True)\n",
        "    for year in YEARS:\n",
        "        zip_path = SEC_ZIP_DIR / f\"{year}.zip\"\n",
        "        if not zip_path.exists():\n",
        "            continue\n",
        "        out_year = SEC_BATCH_ROOT / str(year)\n",
        "        out_year.mkdir(parents=True, exist_ok=True)\n",
        "        existing = list(out_year.glob(f\"{year}_batch_*.parquet\"))\n",
        "        if existing:\n",
        "            continue\n",
        "        if SEC_PARSE_MODE == \"raw\":\n",
        "            process_zip_year_raw_text(zip_path=zip_path, out_dir=out_year, batch_max_rows=1000, batch_max_text_bytes=250 * 1024 * 1024, encoding=\"utf-8\", **common)\n",
        "        else:\n",
        "            process_zip_year(zip_path=zip_path, out_dir=out_year, batch_max_rows=2000, batch_max_text_bytes=250 * 1024 * 1024, header_search_limit=8000, encoding=\"utf-8\", **common)\n",
        "else:\n",
        "    print(\"RUN_SEC_PARSE=False; using existing SEC batches.\")\n",
        "\n",
        "if RUN_SEC_YEARLY_MERGE:\n",
        "    merge_yearly_batches(batch_dir=SEC_BATCH_ROOT, out_dir=SEC_YEAR_MERGED_DIR, checkpoint_path=SEC_YEAR_MERGED_DIR / \"done_years.json\", local_work_dir=LOCAL_MERGE_WORK, batch_size=128_000, compression=\"zstd\", compression_level=1, validate_inputs=\"full\", years=[str(y) for y in YEARS])\n",
        "\n",
        "sec_summaries = summarize_year_parquets(SEC_YEAR_MERGED_DIR)\n",
        "ok_files = [Path(r[\"path\"]) for r in sec_summaries if r.get(\"status\") == \"OK\"]\n",
        "if not ok_files:\n",
        "    raise ValueError(\"No OK SEC yearly parquet files found.\")\n",
        "build_light_metadata_dataset(parquet_dir=ok_files, out_path=SEC_LIGHT_METADATA_PATH, drop_columns=(\"full_text\",), sort_columns=(\"file_date_filename\", \"cik\"), compression=\"zstd\")\n",
        "print({\"ok_year_files\": len(ok_files), \"light_path\": str(SEC_LIGHT_METADATA_PATH)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Prepare SEC pre-merge input\n",
        "\n",
        "Required columns: `doc_id`, `cik_10`, `filing_date`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "year_files = sorted([p for p in SEC_YEAR_MERGED_DIR.glob(\"*.parquet\") if p.stem.isdigit() and len(p.stem) == 4])\n",
        "if not year_files:\n",
        "    raise ValueError(f\"No yearly SEC files found in {SEC_YEAR_MERGED_DIR}\")\n",
        "\n",
        "sec_raw_lf = pl.scan_parquet(year_files)\n",
        "sec_schema = sec_raw_lf.collect_schema()\n",
        "for c in (\"doc_id\", \"cik_10\"):\n",
        "    if c not in sec_schema:\n",
        "        raise ValueError(f\"Missing required SEC column: {c}\")\n",
        "\n",
        "if \"filing_date\" in sec_schema and \"file_date_filename\" in sec_schema:\n",
        "    filing_date_expr = pl.coalesce([pl.col(\"filing_date\").cast(pl.Date, strict=False), pl.col(\"file_date_filename\").cast(pl.Date, strict=False)]).alias(\"filing_date\")\n",
        "elif \"filing_date\" in sec_schema:\n",
        "    filing_date_expr = pl.col(\"filing_date\").cast(pl.Date, strict=False).alias(\"filing_date\")\n",
        "elif \"file_date_filename\" in sec_schema:\n",
        "    filing_date_expr = pl.col(\"file_date_filename\").cast(pl.Date, strict=False).alias(\"filing_date\")\n",
        "else:\n",
        "    raise ValueError(\"Missing both filing_date and file_date_filename.\")\n",
        "\n",
        "optional_cols = [c for c in (\"document_type_filename\", \"form_type\", \"period_end\", \"acceptance_datetime\", \"accession_number\", \"accession_nodash\") if c in sec_schema]\n",
        "sec_premerge_input_lf = sec_raw_lf.with_columns(pl.col(\"doc_id\").cast(pl.Utf8, strict=False), pl.col(\"cik_10\").cast(pl.Utf8, strict=False), filing_date_expr).select(\"doc_id\", \"cik_10\", \"filing_date\", *optional_cols)\n",
        "\n",
        "null_dates = sec_premerge_input_lf.select(pl.col(\"filing_date\").is_null().sum()).collect().item()\n",
        "if null_dates > 0:\n",
        "    raise ValueError(f\"Null filing_date rows after fallback: {null_dates}\")\n",
        "\n",
        "print({\"rows\": sec_premerge_input_lf.select(pl.len()).collect().item(), \"doc_ids\": sec_premerge_input_lf.select(pl.col(\"doc_id\").n_unique()).collect().item(), \"optional_cols\": optional_cols})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) SEC-CCM pre-merge (must run before extraction)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sec_ccm_paths: dict[str, Path] | None = None\n",
        "\n",
        "if RUN_SEC_CCM_PREMERGE:\n",
        "    join_spec = SecCcmJoinSpecV1(\n",
        "        alignment_policy=\"NEXT_TRADING_DAY_STRICT\",\n",
        "        daily_join_enabled=True,\n",
        "        daily_join_source=\"MERGED_DAILY_PANEL\",\n",
        "        daily_permno_col=resolved_permno_col,\n",
        "        daily_date_col=resolved_date_col,\n",
        "        daily_feature_columns=tuple(DAILY_FEATURE_COLUMNS),\n",
        "        required_daily_non_null_features=tuple(REQUIRED_DAILY_NON_NULL_FEATURES),\n",
        "    )\n",
        "\n",
        "    sec_ccm_paths = run_sec_ccm_premerge_pipeline(\n",
        "        sec_filings_lf=sec_premerge_input_lf,\n",
        "        link_universe_lf=link_universe_lf,\n",
        "        trading_calendar_lf=trading_calendar_lf,\n",
        "        output_dir=SEC_CCM_OUTPUT_DIR,\n",
        "        daily_lf=ccm_daily_lf,\n",
        "        join_spec=join_spec,\n",
        "        emit_run_report=True,\n",
        "    )\n",
        "\n",
        "    for k in sorted(sec_ccm_paths):\n",
        "        print(f\"{k}: {sec_ccm_paths[k]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if sec_ccm_paths is not None:\n",
        "    ms = pl.read_parquet(sec_ccm_paths[\"sec_ccm_match_status\"])\n",
        "    print(ms.group_by(\"match_reason_code\").agg(pl.len().alias(\"n_docs\")).sort(\"n_docs\", descending=True))\n",
        "    total = ms.height\n",
        "    matched = int(ms.select(pl.col(\"match_flag\").cast(pl.Int64).sum()).item())\n",
        "    acceptance = int(ms.select(pl.col(\"has_acceptance_datetime\").cast(pl.Int64).sum()).item())\n",
        "    print({\"total_docs\": total, \"matched_docs\": matched, \"matched_rate\": (matched / total) if total else 0.0, \"acceptance_coverage\": (acceptance / total) if total else 0.0})\n",
        "    print(\"run_report:\", sec_ccm_paths.get(\"sec_ccm_run_report\"))\n",
        "    print(\"run_dag_mermaid:\", sec_ccm_paths.get(\"sec_ccm_run_dag_mermaid\"))\n",
        "    print(\"run_dag_dot:\", sec_ccm_paths.get(\"sec_ccm_run_dag_dot\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Gated item extraction (matched-first)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analysis_item_paths: list[Path] = []\n",
        "diagnostic_item_paths: list[Path] = []\n",
        "\n",
        "if RUN_GATED_ITEM_EXTRACTION:\n",
        "    if sec_ccm_paths is None:\n",
        "        raise RuntimeError(\"Run SEC-CCM pre-merge first.\")\n",
        "\n",
        "    analysis_item_paths = process_year_dir_extract_items_gated(\n",
        "        year_dir=SEC_YEAR_MERGED_DIR,\n",
        "        out_dir=SEC_ITEMS_ANALYSIS_DIR,\n",
        "        doc_id_allowlist=sec_ccm_paths[\"sec_ccm_analysis_doc_ids\"],\n",
        "        years=[str(y) for y in YEARS],\n",
        "        parquet_batch_rows=16,\n",
        "        out_batch_max_rows=50_000,\n",
        "        out_batch_max_text_bytes=250 * 1024 * 1024,\n",
        "        tmp_dir=LOCAL_TMP,\n",
        "        compression=\"zstd\",\n",
        "        local_work_dir=LOCAL_ITEM_WORK,\n",
        "        non_item_diagnostic=False,\n",
        "        include_full_text=False,\n",
        "        regime=True,\n",
        "        extraction_regime=ITEM_EXTRACTION_REGIME,\n",
        "    )\n",
        "    print({\"analysis_year_files\": len(analysis_item_paths)})\n",
        "\n",
        "    if RUN_UNMATCHED_DIAGNOSTIC_TRACK:\n",
        "        diagnostic_item_paths = process_year_dir_extract_items_gated(\n",
        "            year_dir=SEC_YEAR_MERGED_DIR,\n",
        "            out_dir=SEC_ITEMS_DIAGNOSTIC_DIR,\n",
        "            doc_id_allowlist=sec_ccm_paths[\"sec_ccm_diagnostic_doc_ids\"],\n",
        "            years=[str(y) for y in YEARS],\n",
        "            parquet_batch_rows=16,\n",
        "            out_batch_max_rows=50_000,\n",
        "            out_batch_max_text_bytes=250 * 1024 * 1024,\n",
        "            tmp_dir=LOCAL_TMP,\n",
        "            compression=\"zstd\",\n",
        "            local_work_dir=LOCAL_ITEM_WORK,\n",
        "            non_item_diagnostic=False,\n",
        "            include_full_text=False,\n",
        "            regime=True,\n",
        "            extraction_regime=ITEM_EXTRACTION_REGIME,\n",
        "        )\n",
        "        print({\"diagnostic_year_files\": len(diagnostic_item_paths)})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) No-item diagnostics + boundary diagnostics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analysis_no_item: list[tuple[str, Path, Path]] = []\n",
        "if RUN_NO_ITEM_DIAGNOSTICS and RUN_GATED_ITEM_EXTRACTION:\n",
        "    out_dir = SEC_NO_ITEM_DIR / \"analysis\"\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for item_path in analysis_item_paths:\n",
        "        year = item_path.stem\n",
        "        filing_path = SEC_YEAR_MERGED_DIR / f\"{year}.parquet\"\n",
        "        if not filing_path.exists():\n",
        "            continue\n",
        "        out_no_item = out_dir / f\"{year}_no_item_filings.parquet\"\n",
        "        out_stats = out_dir / f\"{year}_no_item_stats.csv\"\n",
        "        compute_no_item_diagnostics(filing_path, item_path, out_no_item, out_stats, include_full_text=False)\n",
        "        analysis_no_item.append((year, out_no_item, out_stats))\n",
        "print({\"analysis_no_item_years\": len(analysis_no_item)})\n",
        "\n",
        "boundary_results = None\n",
        "if RUN_BOUNDARY_DIAGNOSTICS:\n",
        "    if sec_ccm_paths is None:\n",
        "        raise RuntimeError(\"Run SEC-CCM pre-merge first.\")\n",
        "\n",
        "    allow_lf = pl.scan_parquet(sec_ccm_paths[\"sec_ccm_analysis_doc_ids\"]).select(pl.col(\"doc_id\").cast(pl.Utf8)).drop_nulls(subset=[\"doc_id\"]).unique(subset=[\"doc_id\"])\n",
        "    staged = 0\n",
        "    for year in YEARS:\n",
        "        src = SEC_YEAR_MERGED_DIR / f\"{year}.parquet\"\n",
        "        if not src.exists():\n",
        "            continue\n",
        "        dst = BOUNDARY_INPUT_DIR / src.name\n",
        "        pl.scan_parquet(src).join(allow_lf, on=\"doc_id\", how=\"semi\").sink_parquet(dst, compression=\"zstd\")\n",
        "        staged += 1\n",
        "\n",
        "    diag_config = DiagnosticsConfig(\n",
        "        parquet_dir=BOUNDARY_INPUT_DIR,\n",
        "        out_path=BOUNDARY_OUT_DIR / \"suspicious_boundaries_matched.csv\",\n",
        "        report_path=BOUNDARY_OUT_DIR / \"suspicious_boundaries_matched_report.txt\",\n",
        "        samples_dir=BOUNDARY_OUT_DIR / \"samples\",\n",
        "        batch_size=8,\n",
        "        max_files=0,\n",
        "        max_examples=50,\n",
        "        emit_manifest=True,\n",
        "        manifest_items_path=BOUNDARY_OUT_DIR / \"manifest_items.csv\",\n",
        "        manifest_filings_path=BOUNDARY_OUT_DIR / \"manifest_filings.csv\",\n",
        "        sample_pass=100,\n",
        "        sample_seed=42,\n",
        "        sample_filings_path=BOUNDARY_OUT_DIR / \"sample_filings.csv\",\n",
        "        sample_items_path=BOUNDARY_OUT_DIR / \"sample_items.csv\",\n",
        "        emit_html=True,\n",
        "        html_out=BOUNDARY_OUT_DIR / \"html\",\n",
        "        html_scope=\"sample\",\n",
        "        extraction_regime=\"v2\",\n",
        "        diagnostics_regime=\"v2\",\n",
        "        target_set=\"cohen2020_common\",\n",
        "        focus_items=parse_focus_items(None),\n",
        "        report_item_scope=\"target\",\n",
        "    )\n",
        "    print({\"boundary_staged_year_files\": staged})\n",
        "    boundary_results = run_boundary_diagnostics(diag_config)\n",
        "    print(boundary_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Failure modes and diagnostics checklist\n",
        "\n",
        "- Missing SEC pre-merge columns (`doc_id`, `cik_10`, date fields) -> fail fast with explicit error.\n",
        "- Missing/invalid `filing_date` after fallback -> fail fast.\n",
        "- Weak or empty CCM link-universe coverage -> inspect link-universe counts before pre-merge.\n",
        "- Non-unique `doc_id` issues -> validated against match-status row/uniqueness checks.\n",
        "- CCM daily schema mismatch (permno/date/features) -> explicit source column resolution and validation.\n",
        "- Low match rate -> inspect `sec_ccm_unmatched_diagnostics.parquet` and `sec_ccm_run_report.md`.\n",
        "- Empty gated extraction unexpectedly -> compare allowlist counts vs extracted doc_id counts.\n",
        "- Boundary diagnostics runtime too high -> reduce year scope or set max-files/sample controls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook test scenarios\n",
        "\n",
        "1. Smoke run in `REUSE` mode on sample data: verify SEC-CCM artifacts and run report/DAG files exist.\n",
        "2. Functional run in `REBUILD` mode: verify CCM daily parquet is produced and consumed by pre-merge.\n",
        "3. Gating correctness: verify extracted analysis doc_ids are a subset of analysis allowlist.\n",
        "4. Pre-merge invariants: one row per `doc_id` in `sec_ccm_match_status.parquet`.\n",
        "5. Diagnostics presence: boundary CSV/report/HTML outputs exist when enabled.\n",
        "6. Schema checks: `kypermno` Int32 and `data_status` UInt64 non-null.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Validation + artifact index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_rows: list[dict[str, object]] = []\n",
        "\n",
        "if RUN_VALIDATION_CHECKS and sec_ccm_paths is not None:\n",
        "    pre = sec_premerge_input_lf.select(pl.len().alias(\"rows\"), pl.col(\"doc_id\").n_unique().alias(\"uniq\")).collect().row(0, named=True)\n",
        "    ms_lf = pl.scan_parquet(sec_ccm_paths[\"sec_ccm_match_status\"])\n",
        "    ms = ms_lf.select(pl.len().alias(\"rows\"), pl.col(\"doc_id\").n_unique().alias(\"uniq\")).collect().row(0, named=True)\n",
        "    if pre[\"rows\"] != ms[\"rows\"]:\n",
        "        raise AssertionError(f\"premerge rows {pre['rows']} != match_status rows {ms['rows']}\")\n",
        "    if ms[\"rows\"] != ms[\"uniq\"]:\n",
        "        raise AssertionError(\"sec_ccm_match_status is not unique on doc_id\")\n",
        "\n",
        "    schema = pl.scan_parquet(sec_ccm_paths[\"final_flagged_data\"]).collect_schema()\n",
        "    if schema.get(\"kypermno\") != pl.Int32:\n",
        "        raise AssertionError(f\"kypermno dtype not Int32: {schema.get('kypermno')}\")\n",
        "    if schema.get(\"data_status\") != pl.UInt64:\n",
        "        raise AssertionError(f\"data_status dtype not UInt64: {schema.get('data_status')}\")\n",
        "    null_status = pl.scan_parquet(sec_ccm_paths[\"final_flagged_data\"]).select(pl.col(\"data_status\").is_null().sum()).collect().item()\n",
        "    if null_status != 0:\n",
        "        raise AssertionError(f\"data_status null count: {null_status}\")\n",
        "\n",
        "    validation_rows.append({\"check\": \"premerge_vs_match_status_rows\", \"ok\": True, \"details\": f\"rows={pre['rows']}\"})\n",
        "    validation_rows.append({\"check\": \"match_status_doc_id_unique\", \"ok\": True, \"details\": f\"unique={ms['uniq']}\"})\n",
        "\n",
        "if RUN_VALIDATION_CHECKS and RUN_GATED_ITEM_EXTRACTION and analysis_item_paths and sec_ccm_paths is not None:\n",
        "    allow_lf = pl.scan_parquet(sec_ccm_paths[\"sec_ccm_analysis_doc_ids\"]).select(pl.col(\"doc_id\").cast(pl.Utf8)).drop_nulls(subset=[\"doc_id\"]).unique(subset=[\"doc_id\"])\n",
        "    extracted_lf = pl.scan_parquet([str(p) for p in analysis_item_paths]).select(pl.col(\"doc_id\").cast(pl.Utf8)).drop_nulls(subset=[\"doc_id\"]).unique(subset=[\"doc_id\"])\n",
        "    outside = extracted_lf.join(allow_lf, on=\"doc_id\", how=\"anti\").select(pl.len()).collect().item()\n",
        "    if outside != 0:\n",
        "        raise AssertionError(f\"Extracted doc_ids outside analysis allowlist: {outside}\")\n",
        "    validation_rows.append({\"check\": \"analysis_items_subset_allowlist\", \"ok\": True, \"details\": \"outside=0\"})\n",
        "\n",
        "print(pl.DataFrame(validation_rows) if validation_rows else \"No validations executed\")\n",
        "\n",
        "\n",
        "def _row_count(path: Path) -> int | None:\n",
        "    if not path.exists() or path.suffix.lower() != \".parquet\":\n",
        "        return None\n",
        "    return int(pl.scan_parquet(path).select(pl.len()).collect().item())\n",
        "\n",
        "artifact_rows: list[dict[str, object]] = []\n",
        "\n",
        "def _add(stage: str, key: str, path: Path) -> None:\n",
        "    artifact_rows.append({\"stage\": stage, \"artifact\": key, \"path\": str(path), \"exists\": path.exists(), \"rows\": _row_count(path)})\n",
        "\n",
        "if ccm_daily_path is not None:\n",
        "    _add(\"ccm\", \"ccm_daily_path\", ccm_daily_path)\n",
        "if sec_ccm_paths is not None:\n",
        "    for k in sorted(sec_ccm_paths):\n",
        "        _add(\"sec_ccm\", k, Path(sec_ccm_paths[k]))\n",
        "for p in analysis_item_paths:\n",
        "    _add(\"items_analysis\", p.stem, p)\n",
        "for p in diagnostic_item_paths:\n",
        "    _add(\"items_diagnostic\", p.stem, p)\n",
        "for y, p_no, p_csv in analysis_no_item:\n",
        "    _add(\"no_item_analysis\", f\"{y}_no_item_filings\", p_no)\n",
        "    _add(\"no_item_analysis\", f\"{y}_no_item_stats\", p_csv)\n",
        "for k, p in {\n",
        "    \"boundary_csv\": BOUNDARY_OUT_DIR / \"suspicious_boundaries_matched.csv\",\n",
        "    \"boundary_report\": BOUNDARY_OUT_DIR / \"suspicious_boundaries_matched_report.txt\",\n",
        "    \"boundary_manifest_items\": BOUNDARY_OUT_DIR / \"manifest_items.csv\",\n",
        "    \"boundary_manifest_filings\": BOUNDARY_OUT_DIR / \"manifest_filings.csv\",\n",
        "    \"boundary_html\": BOUNDARY_OUT_DIR / \"html\",\n",
        "}.items():\n",
        "    _add(\"boundary\", k, p)\n",
        "\n",
        "print(pl.DataFrame(artifact_rows).sort([\"stage\", \"artifact\"]) if artifact_rows else \"No artifacts indexed\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
